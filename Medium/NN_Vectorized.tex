% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\author{}
\date{}

\begin{document}

\hypertarget{header-n86}{%
\section{Understanding a Simple Neural Networks Learning for Multi-Class
Classification -Maths Version}\label{header-n86}}

\begin{figure}
\centering
\includegraphics{/Users/anupam7936/afb/Blogs/NN_Vectorized.assets/image-20200420215951820.png}
\caption{image-20200420215951820}
\end{figure}

We have learned through a simple Neural Networks for Binary
Classification in a separate blogs. Now we will try to understand the
working of Neural Networks for Multi-Class Classification. For the
consistency of the paramters names across all layers we have used 'h' at
the outer layer too.

Summarizing Feed Forward parameters for Multi-Class Classification:

\hypertarget{header-n157}{%
\subsection{Hidden Layer 1}\label{header-n157}}

\begin{longtable}[]{@{}lll@{}}
\toprule
Input/Output Parameters & Parameter Expressions & Parameter
Description\tabularnewline
\midrule
\endhead
\(\large X_1\) &
\(\large\begin{bmatrix} x_{1} \\ x_{2} \\ . \\ x_n\\\end{bmatrix}\quad\)
& Input vector (different features)\tabularnewline
\(\large W_1\) &
\(\large\begin{bmatrix} w_{111} & w_{112} & ... &w_{11n}  \\ w_{121} & w_{122} & ... &w_{12n} \\ ... & ... & ... &... \\ w_{1m1} & w_{1m} & ... &w_{1mn}\\ \end{bmatrix} \quad\)
& \(W_1\) matrix size is m x n, where, m = number of inputs and n =
number of neurons\tabularnewline
\(\large b_1\) &
\(\large\begin{bmatrix} b_{11}  \\ b_{12}  \\ ...  \\ b_{1n} \\ \end{bmatrix} \quad\)
& \(b_1\) is a vector with size as n, number of neuron in layer
1\tabularnewline
\(\large a_{1n}\) &
\(\large\begin{bmatrix} a_{11}  \\ a_{12}  \\ ...  \\ a_{1n} \\ \end{bmatrix} \quad = W_1 * X_1 + b_1\)
& A pre-activation function vector of size n, number of neurons in layer
1. The Matrix multiplication size will be, {[}n * m{]} * {[}m * 1 {]} +
{[}n * 1{]} = {[}n*1{]}\tabularnewline
\(\large h_{1n}\) &
\(\large \begin{bmatrix} h_{11}  \\ h_{12}  \\ ...  \\ h_{1n} \\ \end{bmatrix} \quad =\begin{bmatrix} g(a_{11})  \\ g(a_{12})  \\ ...  \\ g(a_{1n}) \\ \end{bmatrix} \quad\)
& A activation function vector of size n, number of neurons in layer 1.
The activation can be any non-linear continous function like Sigmoid,
tanh etc.\tabularnewline
\bottomrule
\end{longtable}

If the function to be considered is Sigmoid then,

\(\Large g(a_{11}) = \frac{1}{1+ e ^{-a_{11}}}\) and so on.

\hypertarget{header-n332}{%
\subsubsection{Hidden Layer 2}\label{header-n332}}

\begin{longtable}[]{@{}lll@{}}
\toprule
Input/Output Parameters & Parameter Expressions & Parameter
Description\tabularnewline
\midrule
\endhead
\(\large W_2\) &
\(\large\begin{bmatrix} w_{211} & w_{212} & ... &w_{21n}  \\ w_{221} & w_{222} & ... &w_{22n} \\ ... & ... & ... &... \\ w_{2m1} & w_{2m2} & ... &w_{2mn}\\ \end{bmatrix} \quad\)
& \(W_2\) matrix size is m x n, where, m = number of previous layer
neurons and n = number of neurons in the current layer\tabularnewline
\(\large b_2\) &
\(\large\begin{bmatrix} b_{21}  \\ b_{22}  \\ ...  \\ b_{2n} \\ \end{bmatrix} \quad\)
& \(b_2\) is a vector with size as n, number of neuron in layer
2\tabularnewline
\(\large a_{2n}\) &
\(\large \begin{bmatrix} a_{21}  \\ a_{22}  \\ ...  \\ a_{2n} \\ \end{bmatrix} \quad = W_2 * h_1 + b_2 \)
& A pre-activation function vector of size n, number of neurons in layer
2. The Matrix multiplication size will be {[}n * m {]} * {[}m * 1{]} +
{[}n*1{]} = {[}n * 1{]}\tabularnewline
\(\large h_{2n}\) &
\(\large \begin{bmatrix} h_{21}  \\ h_{22}  \\ ...  \\ h_{2n} \\ \end{bmatrix} \quad = \begin{bmatrix} g(a_{21})  \\ g(a_{22})  \\ ...  \\ g(a_{2n}) \\ \end{bmatrix} \quad\)
& A activation function vector of size n, number of neurons in layer 2.
The activation can be any non-linear continous function like Sigmoid,
tanh etc.\tabularnewline
\bottomrule
\end{longtable}

If the function to be considered is Sigmoid then,

\(\Large g^{'}(a_{21}) = \frac{1}{1+ e ^{-a_{21}}}\) and so on.

\hypertarget{header-n311}{%
\subsection{Output Layer}\label{header-n311}}

\begin{longtable}[]{@{}l@{}}
\toprule
Input/Output Parameters\tabularnewline
\midrule
\endhead
\(\large W_3\)\tabularnewline
\(\large b_3\) \tabularnewline
\(\large a_{3n}\)\tabularnewline
\(\large h_{3n}\)\tabularnewline
\bottomrule
\end{longtable}

For softmax,

\( \Large  softmax(a_{31})  = \LARGE \frac{e^{a_{31}}}{\sum e ^ {a_i}}\)
where \emph{i}, ranges from 1 to n, n being the number of neurons at the
output layer.

So if you combine the above layer equations it will result into below:

\(\large \hat{y} = f(x) = O(W_3g(W_2g(W_1X_1+b_1) + b_2)+ b_3)\), where
\(O\) can be a softmax or any other non-linear continous function.

CROSS ENTROPY LOSS FUNCTION for Multi-Class Classification is given by:

\(\large L(\Theta)  = -[(1-y)log(1 - \hat{y}) + ylog\hat{y}] \)

\end{document}
