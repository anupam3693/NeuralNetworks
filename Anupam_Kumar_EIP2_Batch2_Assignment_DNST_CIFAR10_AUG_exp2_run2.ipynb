{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K70hAckqg0EA"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/\n",
    "# !pip install -q keras \n",
    "# import keras \n",
    "# print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# batch_size = 32\n",
    "# num_classes =  10\n",
    "# epochs = 50\n",
    "# l = 40\n",
    "# num_filter = 10\n",
    "# compression = 0.5\n",
    "# dropout_rate = 0\n",
    "\n",
    "batch_size = 64\n",
    "num_classes =  10\n",
    "epochs = 100\n",
    "l = 14\n",
    "num_filter = 20\n",
    "compression = 0.9\n",
    "dropout_rate = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB7o3zu1g6eT"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoding \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter, dropout_rate):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter, dropout_rate):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "# Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate=0.2)\n",
    "output = output_layer(Third_Transition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 22814
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "e25a0d0c-f142-4d4f-94d3-006930feaf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 20)   540         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 20)   80          conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 32, 32, 20)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 18)   3240        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 32, 32, 18)   0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 32, 32, 38)   0           conv2d_47[0][0]                  \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 38)   152         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 32, 38)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 18)   6156        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 32, 32, 18)   0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 32, 32, 56)   0           concatenate_43[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 56)   224         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 56)   0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 18)   9072        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 32, 32, 18)   0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 32, 32, 74)   0           concatenate_44[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 74)   296         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 32, 32, 74)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 18)   11988       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 32, 32, 18)   0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 32, 32, 92)   0           concatenate_45[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 92)   368         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 92)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 18)   14904       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 32, 32, 18)   0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 32, 32, 110)  0           concatenate_46[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 110)  440         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 110)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 18)   17820       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 32, 32, 18)   0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 32, 32, 128)  0           concatenate_47[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 128)  512         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 128)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 18)   20736       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 32, 32, 18)   0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 32, 32, 146)  0           concatenate_48[0][0]             \n",
      "                                                                 dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 146)  584         concatenate_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 146)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 32, 32, 18)   23652       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 32, 32, 18)   0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 32, 32, 164)  0           concatenate_49[0][0]             \n",
      "                                                                 dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 32, 32, 164)  656         concatenate_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 32, 32, 164)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 32, 32, 18)   26568       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 32, 32, 18)   0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 32, 32, 182)  0           concatenate_50[0][0]             \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 32, 32, 182)  728         concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 32, 32, 182)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 18)   29484       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 32, 32, 18)   0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 32, 32, 200)  0           concatenate_51[0][0]             \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 200)  800         concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 32, 32, 200)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 18)   32400       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 32, 32, 18)   0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 32, 32, 218)  0           concatenate_52[0][0]             \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 218)  872         concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 218)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 18)   35316       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 32, 32, 18)   0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_54 (Concatenate)    (None, 32, 32, 236)  0           concatenate_53[0][0]             \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 236)  944         concatenate_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 236)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 18)   38232       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 32, 32, 18)   0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 32, 32, 254)  0           concatenate_54[0][0]             \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 254)  1016        concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 32, 32, 254)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 18)   41148       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 32, 32, 18)   0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_56 (Concatenate)    (None, 32, 32, 272)  0           concatenate_55[0][0]             \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 32, 272)  1088        concatenate_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 32, 32, 272)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 18)   4896        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, 32, 32, 18)   0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 16, 16, 18)   0           dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 18)   72          average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 18)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 18)   2916        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, 16, 16, 18)   0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_57 (Concatenate)    (None, 16, 16, 36)   0           average_pooling2d_5[0][0]        \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 36)   144         concatenate_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 36)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 18)   5832        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 16, 16, 18)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_58 (Concatenate)    (None, 16, 16, 54)   0           concatenate_57[0][0]             \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 54)   216         concatenate_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 54)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 18)   8748        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 16, 16, 18)   0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_59 (Concatenate)    (None, 16, 16, 72)   0           concatenate_58[0][0]             \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 72)   288         concatenate_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 72)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 18)   11664       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, 16, 16, 18)   0           conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_60 (Concatenate)    (None, 16, 16, 90)   0           concatenate_59[0][0]             \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 90)   360         concatenate_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 90)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 18)   14580       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 16, 16, 18)   0           conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_61 (Concatenate)    (None, 16, 16, 108)  0           concatenate_60[0][0]             \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 108)  432         concatenate_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 108)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 18)   17496       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 16, 16, 18)   0           conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 16, 16, 126)  0           concatenate_61[0][0]             \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 16, 16, 126)  504         concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 16, 16, 126)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 18)   20412       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 16, 16, 18)   0           conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 16, 16, 144)  0           concatenate_62[0][0]             \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 16, 16, 144)  576         concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 16, 16, 144)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 18)   23328       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, 16, 16, 18)   0           conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 16, 16, 162)  0           concatenate_63[0][0]             \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 16, 16, 162)  648         concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 16, 16, 162)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 18)   26244       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, 16, 16, 18)   0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 16, 16, 180)  0           concatenate_64[0][0]             \n",
      "                                                                 dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 16, 180)  720         concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 16, 180)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 18)   29160       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, 16, 16, 18)   0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 16, 16, 198)  0           concatenate_65[0][0]             \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 16, 198)  792         concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 16, 16, 198)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 16, 16, 18)   32076       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, 16, 16, 18)   0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 16, 16, 216)  0           concatenate_66[0][0]             \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 16, 216)  864         concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 16, 16, 216)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 18)   34992       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 16, 16, 18)   0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 16, 16, 234)  0           concatenate_67[0][0]             \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 234)  936         concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 16, 234)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 18)   37908       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 16, 16, 18)   0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_69 (Concatenate)    (None, 16, 16, 252)  0           concatenate_68[0][0]             \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 16, 252)  1008        concatenate_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 16, 252)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 18)   40824       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 16, 16, 18)   0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_70 (Concatenate)    (None, 16, 16, 270)  0           concatenate_69[0][0]             \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 16, 270)  1080        concatenate_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 16, 270)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 18)   4860        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 16, 16, 18)   0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 8, 8, 18)     0           dropout_75[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 18)     72          average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 18)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 18)     2916        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 8, 8, 18)     0           conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 8, 8, 36)     0           average_pooling2d_6[0][0]        \n",
      "                                                                 dropout_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 36)     144         concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 36)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 18)     5832        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 8, 8, 18)     0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 8, 8, 54)     0           concatenate_71[0][0]             \n",
      "                                                                 dropout_77[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 54)     216         concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 54)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 18)     8748        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 8, 8, 18)     0           conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 8, 8, 72)     0           concatenate_72[0][0]             \n",
      "                                                                 dropout_78[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 72)     288         concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 72)     0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 18)     11664       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 8, 8, 18)     0           conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 8, 8, 90)     0           concatenate_73[0][0]             \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 90)     360         concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 90)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 18)     14580       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 8, 8, 18)     0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_75 (Concatenate)    (None, 8, 8, 108)    0           concatenate_74[0][0]             \n",
      "                                                                 dropout_80[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 108)    432         concatenate_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 108)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 18)     17496       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)            (None, 8, 8, 18)     0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_76 (Concatenate)    (None, 8, 8, 126)    0           concatenate_75[0][0]             \n",
      "                                                                 dropout_81[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 126)    504         concatenate_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 126)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 18)     20412       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)            (None, 8, 8, 18)     0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_77 (Concatenate)    (None, 8, 8, 144)    0           concatenate_76[0][0]             \n",
      "                                                                 dropout_82[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 144)    576         concatenate_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 144)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 18)     23328       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 8, 8, 18)     0           conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_78 (Concatenate)    (None, 8, 8, 162)    0           concatenate_77[0][0]             \n",
      "                                                                 dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 162)    648         concatenate_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 162)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 18)     26244       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 8, 8, 18)     0           conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 8, 8, 180)    0           concatenate_78[0][0]             \n",
      "                                                                 dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 180)    720         concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 180)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 18)     29160       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)            (None, 8, 8, 18)     0           conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 8, 8, 198)    0           concatenate_79[0][0]             \n",
      "                                                                 dropout_85[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 198)    792         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 198)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 18)     32076       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)            (None, 8, 8, 18)     0           conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 8, 8, 216)    0           concatenate_80[0][0]             \n",
      "                                                                 dropout_86[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 216)    864         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 216)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 18)     34992       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)            (None, 8, 8, 18)     0           conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 8, 8, 234)    0           concatenate_81[0][0]             \n",
      "                                                                 dropout_87[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 234)    936         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 234)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 18)     37908       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)            (None, 8, 8, 18)     0           conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 8, 8, 252)    0           concatenate_82[0][0]             \n",
      "                                                                 dropout_88[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 252)    1008        concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 252)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 18)     40824       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)            (None, 8, 8, 18)     0           conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 8, 8, 270)    0           concatenate_83[0][0]             \n",
      "                                                                 dropout_89[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 270)    1080        concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 270)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 18)     4860        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)            (None, 8, 8, 18)     0           conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 4, 4, 18)     0           dropout_90[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 18)     72          average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 4, 4, 18)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 2, 2, 18)     0           activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 72)           0           average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           730         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 965,074\n",
      "Trainable params: 952,018\n",
      "Non-trainable params: 13,056\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.load_weights('DNST_model_weights_s2_v1.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ak - block 3 learning rate\n",
    "\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "            \n",
    "clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "clr = CyclicLR(base_lr=0.01, max_lr=0.1,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.5, patience=50, verbose=1,mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "# decay=10e-4,momentum=0.9\n",
    "sgd = SGD(decay=10e-4,momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ep1v_RxxdR-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "  192/50000 [..............................] - ETA: 25:19 - loss: 0.0420 - acc: 0.9896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.115049). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 90s 2ms/step - loss: 0.0534 - acc: 0.9813 - val_loss: 0.3524 - val_acc: 0.9258\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0494 - acc: 0.9828 - val_loss: 0.3623 - val_acc: 0.9248\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0466 - acc: 0.9831 - val_loss: 0.4286 - val_acc: 0.9173\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0432 - acc: 0.9843 - val_loss: 0.3920 - val_acc: 0.9222\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0382 - acc: 0.9867 - val_loss: 0.3765 - val_acc: 0.9252\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0360 - acc: 0.9871 - val_loss: 0.3767 - val_acc: 0.9265\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0333 - acc: 0.9881 - val_loss: 0.3869 - val_acc: 0.9251\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0337 - acc: 0.9882 - val_loss: 0.3949 - val_acc: 0.9249\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0345 - acc: 0.9879 - val_loss: 0.3969 - val_acc: 0.9257\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0340 - acc: 0.9882 - val_loss: 0.3858 - val_acc: 0.9266\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0300 - acc: 0.9899 - val_loss: 0.3889 - val_acc: 0.9260\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0301 - acc: 0.9893 - val_loss: 0.3874 - val_acc: 0.9277\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0290 - acc: 0.9899 - val_loss: 0.3926 - val_acc: 0.9253\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0283 - acc: 0.9903 - val_loss: 0.3905 - val_acc: 0.9266\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0287 - acc: 0.9903 - val_loss: 0.3937 - val_acc: 0.9266\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0290 - acc: 0.9899 - val_loss: 0.3961 - val_acc: 0.9268\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0288 - acc: 0.9899 - val_loss: 0.4090 - val_acc: 0.9247\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0311 - acc: 0.9885 - val_loss: 0.4238 - val_acc: 0.9256\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0303 - acc: 0.9892 - val_loss: 0.4173 - val_acc: 0.9249\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0290 - acc: 0.9899 - val_loss: 0.3974 - val_acc: 0.9270\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0260 - acc: 0.9913 - val_loss: 0.4048 - val_acc: 0.9262\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0288 - acc: 0.9896 - val_loss: 0.4081 - val_acc: 0.9254\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0325 - acc: 0.9887 - val_loss: 0.4477 - val_acc: 0.9217\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0334 - acc: 0.9884 - val_loss: 0.4169 - val_acc: 0.9283\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0289 - acc: 0.9899 - val_loss: 0.4422 - val_acc: 0.9237\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0267 - acc: 0.9903 - val_loss: 0.4267 - val_acc: 0.9247\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0242 - acc: 0.9916 - val_loss: 0.4286 - val_acc: 0.9264\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0245 - acc: 0.9913 - val_loss: 0.4313 - val_acc: 0.9245\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0265 - acc: 0.9907 - val_loss: 0.4268 - val_acc: 0.9263\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0228 - acc: 0.9922 - val_loss: 0.4176 - val_acc: 0.9271\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.4217 - val_acc: 0.9273\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0228 - acc: 0.9919 - val_loss: 0.4208 - val_acc: 0.9264\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0233 - acc: 0.9918 - val_loss: 0.4176 - val_acc: 0.9270\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0216 - acc: 0.9925 - val_loss: 0.4246 - val_acc: 0.9263\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.4269 - val_acc: 0.9273\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.4226 - val_acc: 0.9279\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0229 - acc: 0.9920 - val_loss: 0.4307 - val_acc: 0.9274\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0232 - acc: 0.9919 - val_loss: 0.4444 - val_acc: 0.9237\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0235 - acc: 0.9918 - val_loss: 0.4401 - val_acc: 0.9267\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 0.0230 - acc: 0.9919 - val_loss: 0.4510 - val_acc: 0.9257\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0218 - acc: 0.9926 - val_loss: 0.4450 - val_acc: 0.9258\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0215 - acc: 0.9924 - val_loss: 0.4369 - val_acc: 0.9280\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0239 - acc: 0.9915 - val_loss: 0.5292 - val_acc: 0.9157\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0274 - acc: 0.9903 - val_loss: 0.4745 - val_acc: 0.9237\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0250 - acc: 0.9909 - val_loss: 0.4719 - val_acc: 0.9234\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0231 - acc: 0.9922 - val_loss: 0.4449 - val_acc: 0.9260\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0205 - acc: 0.9926 - val_loss: 0.4477 - val_acc: 0.9266\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0208 - acc: 0.9925 - val_loss: 0.4536 - val_acc: 0.9250\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0228 - acc: 0.9921 - val_loss: 0.4530 - val_acc: 0.9266\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0216 - acc: 0.9923 - val_loss: 0.4622 - val_acc: 0.9255\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 83s 2ms/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.4544 - val_acc: 0.9258\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "# ak - block 4 - image aug\n",
    "\n",
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = False\n",
    "callbacks_list=[clr,earlystopper]\n",
    "\n",
    "start = time.time()\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model_info = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks_list\n",
    "        )\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in 0 to 180 degrees\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    \n",
    "    model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=2*(x_train.shape[0]//batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks_list\n",
    "                       )\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 1.18 hours to train\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Model took %0.2f hours to train\"%((end - start)/3600))\n",
    "\n",
    "plot_model_history(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "4155982a-83c7-4775-f31f-bcbeaa22309b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 543us/step\n",
      "Test loss: 0.4544227167217876\n",
      "Test accuracy: 0.9258\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "56ded1ff-0d18-4299-dadd-cc9d7005acea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model_weights_s2_v2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Og56VCRh5j8V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Anupam_Kumar_EIP2_Batch2_Assignment_DNST_CIFAR10_AUG_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
